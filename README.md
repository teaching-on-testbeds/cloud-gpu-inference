# Using cloud servers for GPU-based inference

Machine learning models are most often trained in the “cloud”, on powerful centralized servers with specialized resources (like GPU acceleration) for training machine learning models. These servers are also well-resources for inference, i.e. making predictions on new data.

In this experiment, we will use a cloud server equipped with GPU acceleration for fast inference in an image classification context.




To run this experiment on Chameleon, open a terminal inside the Chameleon Jupyter environment and run

```
cd ~/work
git clone https://github.com/teaching-on-testbeds/cloud-gpu-inference
```

Then, open the notebook inside the `cloud-gpu-inference` directory and follow along with the instructions there.

> Note: This experiment assumes that you already have a lease for an RTX6000 GPU server on CHI@UC!

---
This material is based upon work supported by the National Science Foundation under Grant No. 2230079.

